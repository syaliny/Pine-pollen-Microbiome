---
title: "ITS2 Pine pollen"
author: "Syaliny"
date: "16/6/2022"
output: html_document
editor_options: 
  chunk_output_type: inline
---

## R Markdown


```{r cars}
#!/usr/bin/env Rscript

##if (!requireNamespace("BiocManager", quietly = TRUE))
   ## install.packages("BiocManager")
##BiocManager::install(version = '3.13', ask= FALSE)
##BiocManager::install("dada2", version = "3.13")
##BiocManager::install("ShortRead")

library("dada2")
library("ShortRead")

path <- "~/PATH/"
list.files(path)

  #Forward and reverse fastq filenames have format: 
fnFs <- sort(list.files(path, pattern="_L001_R1.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="_L001_R2.fastq.gz", full.names = TRUE))

# Extract sample names
fnFs
sample.names <- sub("(.*?_.*?_.*?)_.*", "\\1", basename(fnFs))
sample.names

```

```{r}
#Primer checks
FWD <- "CTTGGTCATTTAGAGGAAGTAA" 
REV <- "GCTGCGTTCTTCATCGATGC"


allOrients <- function(primer) {
    # Create all orientations of the input sequence
    require(Biostrings)
    dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
    orients <- c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna), 
        RevComp = reverseComplement(dna))
    return(sapply(orients, toString))  # Convert back to character vector
}
FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)
FWD.orients

```

```{r}
path2<- "~/PATH/"
fnFs.filtN <- file.path(path2, "filtN", basename(fnFs)) # Put N-filterd files in filtN/ subdirectory
fnRs.filtN <- file.path(path2, "filtN", basename(fnRs))
filterAndTrim(fnFs, fnFs.filtN, fnRs, fnRs.filtN, maxN = 0, multithread = TRUE)

```

```{r}
primerHits <- function(primer, fn) {
    # Counts number of reads in which the primer is found
    nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
    return(sum(nhits > 0))
}

rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[10]]), 
    FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.filtN[[10]]), 
    REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.filtN[[10]]), 
    REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[10]]))

```


```{r}
# cutadapt path on your machine.  
cutadapt <- path.expand("/bin/cutadapt")

# Make sure it works
system2(cutadapt, args = "--version") 
```

```{r}
path.cut <- file.path(path2, "cutadapt")
if(!dir.exists(path.cut)) dir.create(path.cut)
fnFs.cut <- file.path(path.cut, basename(fnFs))
fnRs.cut <- file.path(path.cut, basename(fnRs))

FWD.RC <- dada2:::rc(FWD)
REV.RC <- dada2:::rc(REV)
# Trim FWD and the reverse-complement of REV off of R1 (forward reads)
R1.flags <- paste("-g", FWD, "-a", REV.RC) 
# Trim REV and the reverse-complement of FWD off of R2 (reverse reads)
R2.flags <- paste("-G", REV, "-A", FWD.RC) 
# Run Cutadapt
for(i in seq_along(fnFs)) {
  system2(cutadapt, args = c(R1.flags, R2.flags, "-n", 2, # -n 2 required to remove FWD and REV from reads
                             "-o", fnFs.cut[i], "-p", fnRs.cut[i], # output files
                             fnFs.filtN[i], fnRs.filtN[i])) # input files
}
```


```{r}
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[10]]), 
    FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.cut[[10]]), 
    REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.cut[[10]]), 
    REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[10]]))
```

```{r}
# Forward and reverse fastq filenames have the format:
cutFs <- sort(list.files(path.cut, pattern = "_L001_R1.fastq.gz", full.names = TRUE))
cutRs <- sort(list.files(path.cut, pattern = "_L001_R2.fastq.gz", full.names = TRUE))

# Extract sample names, assuming filenames have format:
get.sample.name <- function(fname) strsplit(basename(fname), "_K4FTN")[[1]][1]
sample.names <- unname(sapply(cutFs, get.sample.name))
head(sample.names)
```

```{r}
##pdf("~/PATH/Plot_quality_ITS2_pine_pollen.pdf", height=8,width=10)
plotQualityProfile(cutFs[1:4])
plotQualityProfile(cutRs[1:4])
##dev.off()
```

#### Filter and trim
```{r}
filtFs <- file.path(path.cut, "filtered", basename(cutFs))
filtRs <- file.path(path.cut, "filtered", basename(cutRs))

filtered_out <- filterAndTrim(cutFs, filtFs, cutRs, filtRs, maxN = 0, maxEE = c(2, 5), 
    truncQ = 2, minLen = 50, rm.phix = TRUE, compress = TRUE, multithread = TRUE)  # on windows, set multithread = FALSE

head(filtered_out)
```


```{r}
errF <- learnErrors(filtFs, multithread=TRUE)
errR <- learnErrors(filtRs, multithread=TRUE)

plotErrors(errF, nominalQ = TRUE)

```
```{r}
#some reads didnt pass the filter step so need to re-make sample names
path <- "~/PATH/cutadapt/filtered/"
list.files(path) #Check it has files you expect 
filtFs <- sort(list.files(path, pattern="_L001_R1.fastq.gz", full.names = TRUE))
filtRs <- sort(list.files(path, pattern="_L001_R2.fastq.gz", full.names = TRUE)) 

# Extract sample names from filtered files
filtFs
sample.names <- sub("(.*?_.*?_.*?_.*?)_.*", "\\1", basename(filtFs))
sample.names
```


```{r}
# dereplicate reads
derep_filtFs <- derepFastq(filtFs, verbose=TRUE)
names(derep_filtFs) <- sample.names # the sample names in these objects are initially the file names of the samples, this sets them to the sample names for the rest of the workflow
derep_filtRs <- derepFastq(filtRs, verbose=TRUE)
names(derep_filtRs) <- sample.names

# inferring ASV
dadaFs <- dada(derep_filtFs, err=errF, multithread=TRUE)
dadaRs <- dada(derep_filtRs, err=errR, multithread=TRUE)

```

```{r}
# merging ASV
merged_amplicons <- mergePairs(dadaFs, derep_filtFs, dadaRs, derep_filtRs, verbose=TRUE)
seqtab <- makeSequenceTable(merged_amplicons)
reads.per.seqlen <- tapply(colSums(seqtab), factor(nchar(getSequences(seqtab))), sum)
write.table(reads.per.seqlen, "~/PATH/ASV_length.txt")
write.table(seqtab, "~/PATH/merged.txt")

# remove chimeric ASV
seqtab.nochim <- removeBimeraDenovo(seqtab, multithread=TRUE, verbose=TRUE)
write.table(seqtab.nochim, "~/PATH/seqtab_nochim.txt")
```

```{r}
# final summary table
getN <- function(x) sum(getUniques(x))
summary_tab <- data.frame(row.names=sample.names, dada2_input=filtered_out[ ,1],
               filtered=filtered_out[,2], dada_f=sapply(dadaFs, getN),
               dada_r=sapply(dadaRs, getN), merged=sapply(merged_amplicons, getN),
               nonchim=rowSums(seqtab.nochim),
               final_perc_reads_retained=round(rowSums(seqtab.nochim)/filtered_out[,1]*100, 1))
head(summary_tab)
write.table(summary_tab, "~/PATH/final_summary.txt")


# assign taxonomy
taxa <- assignTaxonomy(seqtab.nochim,"~/PATH/sh_general_release_dynamic_10.05.2021.fasta", multithread = TRUE, tryRC = TRUE)

# giving our seq headers more manageable names (ASV_1, ASV_2...)
asv_seqs <- colnames(seqtab.nochim)
asv_headers <- vector(dim(seqtab.nochim)[2], mode="character")

for (i in 1:dim(seqtab.nochim)[2]) {
  asv_headers[i] <- paste(">ASV", i, sep="_")
}

# writing out a fasta of our final ASV seqs:
asv_fasta <- c(rbind(asv_headers, asv_seqs))
write(asv_fasta, "~/PATH/ASVs.fa")

# count table:
asv_tab <- t(seqtab.nochim)
row.names(asv_tab) <- sub(">", "", asv_headers)
write.table(asv_tab, "~/PATH/ASVs_counts.tsv", sep="\t", quote=F, col.names=NA)

# tax table:
asv_tax <- taxa
row.names(asv_tax) <- sub(">", "", asv_headers)
write.table(asv_tax, "~/PATH/ASVs_taxonomy.tsv", sep="\t", quote=F, col.names=NA)
```


